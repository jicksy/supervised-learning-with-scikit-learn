{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering and scaling your data\n",
    "\n",
    "* The performance of a model can improve if the features are scaled. Note that this is not always the case: In the Congressional voting records dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact.\n",
    "\n",
    "\n",
    "* You will now explore scaling for yourself on a new dataset - White Wine Quality! Hugo used the Red Wine Quality dataset in the video. We have used the 'quality' feature of the wine to create a binary target variable: If 'quality' is less than 5, the target variable is 1, and otherwise, it is 0.\n",
    "\n",
    "\n",
    "* The DataFrame has been pre-loaded as df, along with the feature and target variable arrays X and y. Explore it in the IPython Shell. Notice how some features seem to have different units of measurement. 'density', for instance, only takes values between 0 and 1, while 'total sulfur dioxide' has a maximum value of 289. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('white-wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis = 1).values\n",
    "y1 = df['quality'].values \n",
    "# original target variable \n",
    "y = y1 <= 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Unscaled Features: 18.432687072460002\n",
      "Standard Deviation of Unscaled Features: 41.54494764094571\n",
      "Mean of Scaled Features: 2.7314972981668206e-15\n",
      "Standard Deviation of Scaled Features: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Import scale\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale the features: X_scaled\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Print the mean and standard deviation of the unscaled features\n",
    "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
    "\n",
    "# Print the mean and standard deviation of the scaled features\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in the mean and standard deviation of the scaled features compared to the unscaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering and Scaling in a pipeline\n",
    "\n",
    "With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Scaling: 0.7700680272108843\n",
      "Accuracy without Scaling: 0.6979591836734694\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Fit the pipeline to the training set: knn_scaled\n",
    "knn_scaled = pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate and fit a k-NN classifier to the unscaled data\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\n",
    "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It looks like scaling has significantly improved model performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
